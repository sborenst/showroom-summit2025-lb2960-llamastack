= Exploring your local environment


In this Module, we will deploy a Llama-Stack Server, a powerful platform designed to simplify and streamline your development workflow. 

We will take you through the process of setting up our first Llama-Stack Server instance, leveraging the Ollama Local LLM server for seamless local development. By the end of this segment, you will gain hands-on experience with the basics of Llama-Stack Server and be equipped to tackle more complex tasks in subsequent lab segments.

Throughout this Module, we will cover essential topics such as:

* Setting up the Ollama Local LLM server
* Configuring the Llama-Stack Server instance
* Exploring core features and functionality

Let's dive into the world of Llama-Stack Server and get started with our first instance!


[#ollama_setup]
== Local LLM Development with Ollama

In this lab, you will be setting up your own local Large Language Model (LLM) server using Ollama. This approach allows you to experiment with AI coding assistance without relying on external cloud services, giving you full control over your AI environment.

Ollama provides a simple way to run various open-source large language models on your local machine. We have taken the libery to install ollama server for you and download a few models to save you time. 

Let's do a quick walkthrough of our environment to understand what's available:

. Run the `ollama list` command to display the models available locally:
+
[source,sh,role=execute]
----
ollama list
----
+
* You should see a list of models that are already downloaded and available for use.
+
[source,textinfo]
----
[dev@dev ~]$ ollama list
NAME                 ID              SIZE      MODIFIED    
granite3.2:latest    9bcb3335083f    4.9 GB    2 hours ago  
----

. Let's pull down the "ollama pull llama3.2:3b-instruct-fp16" model with the `ollama pull` command:
+
[source,sh,role=execute]
----
ollama pull llama3.2:3b-instruct-fp16
----
+
* You should see the model being pulled down, that might take a moment:
+
[source,textinfo]
----
[dev@dev ~]$ ollama pull llama3.2:3b-instruct-fp16
> 
pulling manifest 
pulling e2f46f5b501c... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████▏ 6.4 GB                         
pulling 966de95ca8a6... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████▏ 1.4 KB                         
pulling fcc5a6bec9da... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████▏ 7.7 KB                         
pulling a70ff7e570d9... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████▏ 6.0 KB                         
pulling 56bb8bd477a5... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████▏   96 B                         
pulling 1bc315994ceb... 100% ▕██████████████████████████████████████████████████████████████████████████████████████████████████▏  558 B                         
verifying sha256 digest 
writing manifest 
success 
----

. Running `ollama run {Model_name}` command will let us interact with our LLM on the command line, the `--keepalive 60m` will make sure the model stays loaded, this is important for us as we plan to use this model throughout the lab.
+
[source,sh,role=execute]
----
ollama run llama3.2:3b-instruct-fp16 --keepalive 60m
----
+
* Ask the model a question or two, and type `/bye` or CTRL+D to exist the chat 

[#llamastack_local_server]
== Setting up your first LLamaStack Server


//TODO: Note to Tony, It's slick to have variables, but for the student, is it better to just have a single command?

export LLAMA_STACK_MODEL="meta-llama/Llama-3.2-3B-Instruct"
export INFERENCE_MODEL="meta-llama/Llama-3.2-3B-Instruct"
export LLAMA_STACK_PORT=8321
export LLAMA_STACK_SERVER=http://localhost:$LLAMA_STACK_PORT
export LLAMA_SERVER_CONTAINER_IMAGE="llamastack/distribution-ollama:0.1.9"
//TODO: Need to lock this down the the version we want, and have it ready in our registry (also, pre-loaded in the VM maybe)
docker pull llamastack/distribution-ollama:0.1.9

//TODO: removed this line from the podman command: "  -v ~/.llama:/root/.llama \" do we need it?
podman run -it \
  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \
  $LLAMA_SERVER_CONTAINER_IMAGE \
  --port $LLAMA_STACK_PORT \
  --env INFERENCE_MODEL=$LLAMA_STACK_MODEL \
  --env OLLAMA_URL=http://host.containers.internal:11434


